{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ³ Pemodelan Klasifikasi â€” Deep Dive Decision Tree (Pohon Keputusan)\n",
    "\n",
    "### Jembatan Konseptual\n",
    "\n",
    "Selamat datang di bagian **1.c** dari seri pemodelan kita. Setelah pada bagian sebelumnya kita menjelajahi model berbasis jarak seperti **KNN**, kini kita beralih ke paradigma yang sama sekali berbeda: **model berbasis aturan (rule-based)**. Kita akan melakukan analisis mendalam pada **Decision Tree (Pohon Keputusan)**.\n",
    "\n",
    "### Filosofi Decision Tree: \"Serangkaian Pertanyaan Cerdas\"\n",
    "\n",
    "Bayangkan Decision Tree seperti seorang detektif yang mengajukan serangkaian pertanyaan \"ya/tidak\" yang paling efisien untuk sampai pada sebuah kesimpulan. Model ini bekerja dengan cara memecah data secara rekursif menjadi subset-subset yang lebih kecil dan lebih murni.\n",
    "\n",
    "* **Akar (Root Node):** Titik awal yang berisi seluruh dataset.\n",
    "* **Cabang (Branch):** Mewakili sebuah aturan atau keputusan (misalnya, \"apakah `petal_length` <= 2.45 cm?\").\n",
    "* **Daun (Leaf Node):** Titik akhir yang memberikan hasil klasifikasi (misalnya, \"Spesies adalah setosa\").\n",
    "\n",
    "Keunggulan terbesarnya adalah **interpretabilitas**â€”kita bisa dengan mudah melihat alur \"pemikiran\" model. Namun, tantangan terbesarnya adalah kecenderungan untuk **overfitting**, di mana model menjadi terlalu kompleks dan menghafal data latihan.\n",
    "\n",
    "Di notebook ini, kita akan membangun, memvisualisasikan, mengevaluasi, dan melakukan *pruning* (pemangkasan) pada model Decision Tree untuk mendapatkan model yang tidak hanya akurat, tetapi juga robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 1: Setup Mandiri & Persiapan Data Lengkap\n",
    "\n",
    "Seperti biasa, sel pertama ini akan menangani semua yang kita butuhkan: mengimpor pustaka, memuat data mentah, dan menerapkan seluruh alur pra-pemrosesan yang telah kita sepakati (standarisasi dan penghapusan outlier dengan LOF), hingga membagi data menjadi set pelatihan dan pengujian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# SETUP MANDIRI UNTUK PEMODELAN DECISION TREE\n",
    "# =======================================================\n",
    "\n",
    "# 1. Import Pustaka\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Pustaka yang dibutuhkan telah diimpor.\")\n",
    "\n",
    "# 2. Memuat dan Membuat DataFrame Awal\n",
    "iris = load_iris()\n",
    "df_full = pd.DataFrame(data=iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "df_full['species'] = [iris.target_names[i] for i in iris.target]\n",
    "print(\"Dataset Iris mentah berhasil dibuat.\")\n",
    "\n",
    "# 3. Pra-Pemrosesan: Standarisasi & Penghapusan Outlier dengan LOF\n",
    "print(\"\\nMemulai pra-pemrosesan...\")\n",
    "# Penskalaan (diperlukan untuk LOF)\n",
    "features_to_scale = df_full[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_to_scale)\n",
    "\n",
    "# Deteksi Outlier dengan LOF\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "outlier_labels = lof.fit_predict(features_scaled)\n",
    "df_full['outlier_lof'] = outlier_labels\n",
    "\n",
    "# Menghapus Outlier\n",
    "df_processed = df_full[df_full['outlier_lof'] == 1].copy()\n",
    "df_processed = df_processed.drop(columns=['outlier_lof'])\n",
    "print(f\"Penghapusan outlier selesai. Sisa data: {df_processed.shape[0]} baris.\")\n",
    "\n",
    "# 4. Pra-Pemrosesan: Label Encoding pada Target\n",
    "encoder = LabelEncoder()\n",
    "df_processed['species_encoded'] = encoder.fit_transform(df_processed['species'])\n",
    "print(\"Label encoding untuk variabel target selesai.\")\n",
    "\n",
    "# 5. Memisahkan Fitur (X) dan Target (y)\n",
    "# Catatan: Decision Tree tidak wajib menggunakan data yang diskalakan, jadi kita bisa gunakan data asli yang sudah bersih.\n",
    "X = df_processed[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y = df_processed['species_encoded']\n",
    "\n",
    "# 6. Pembagian Data (Train-Test Split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"\\nPembagian data menjadi set latih dan uji selesai.\")\n",
    "print(f\"Ukuran X_train: {X_train.shape}\")\n",
    "print(f\"Ukuran X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 2: Membangun & Visualisasi Model Decision Tree Dasar\n",
    "\n",
    "Pertama, kita akan membangun model tanpa batasan apapun pada pertumbuhannya. Ini akan menunjukkan kepada kita bagaimana pohon keputusan secara alami mencoba untuk menjadi \"sempurna\" pada data latihan, yang seringkali mengarah ke overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi dan latih model Decision Tree dasar\n",
    "tree_base = DecisionTreeClassifier(random_state=42)\n",
    "tree_base.fit(X_train, y_train)\n",
    "\n",
    "# Visualisasi pohon yang kompleks\n",
    "plt.figure(figsize=(25, 15))\n",
    "plot_tree(tree_base, \n",
    "          feature_names=X.columns,\n",
    "          class_names=encoder.classes_,\n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Visualisasi Decision Tree Dasar (Full Depth)\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analisis Visual:**\n",
    "Seperti yang terlihat, pohon keputusan ini tumbuh sangat dalam dan memiliki banyak cabang. Ia menciptakan aturan-aturan yang sangat spesifik hanya untuk memastikan setiap sampel di data latihan terklasifikasi dengan benar. Ini adalah tanda visual yang jelas dari **overfitting**. Model seperti ini mungkin akan bekerja buruk pada data baru yang belum pernah ia lihat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 3: Pruning â€” Mencegah Overfitting dengan `max_depth`\n",
    "\n",
    "Untuk mengatasi overfitting, kita perlu melakukan **pruning** (pemangkasan). Cara paling sederhana adalah *pre-pruning*, yaitu dengan menghentikan pertumbuhan pohon lebih awal. Kita akan menggunakan hyperparameter `max_depth` untuk membatasi kedalaman pohon.\n",
    "\n",
    "Membatasi kedalaman akan memaksa model untuk hanya mempelajari pola-pola yang paling penting dan umum, sehingga meningkatkan kemampuannya untuk generalisasi ke data baru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi dan latih model Decision Tree yang sudah di-pruning\n",
    "tree_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_pruned.fit(X_train, y_train)\n",
    "\n",
    "# Visualisasi pohon yang lebih sederhana\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(tree_pruned, \n",
    "          feature_names=X.columns,\n",
    "          class_names=encoder.classes_,\n",
    "          filled=True, \n",
    "          rounded=True)\n",
    "plt.title(\"Visualisasi Decision Tree Setelah Pruning (max_depth=3)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analisis Visual (Pruned):**\n",
    "Jauh lebih baik! Pohon ini sangat mudah dibaca dan diinterpretasikan. Kita bisa mengikuti alur keputusannya dari akar hingga daun dengan mudah. Contohnya, aturan pertama di akar adalah `petal_width <= 0.8`. Jika `True`, maka dipastikan itu adalah `setosa`.\n",
    "\n",
    "Sekarang, mari kita evaluasi performa model yang lebih sederhana ini pada data uji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakukan prediksi menggunakan model yang sudah di-pruning\n",
    "y_pred_pruned = tree_pruned.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "print(\"--- Laporan Klasifikasi Model Decision Tree (Pruned, max_depth=3) ---\")\n",
    "print(classification_report(y_test, y_pred_pruned, target_names=encoder.classes_))\n",
    "\n",
    "# Visualisasi Confusion Matrix\n",
    "cm_pruned = confusion_matrix(y_test, y_pred_pruned)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_pruned, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.title('Confusion Matrix - Decision Tree (Pruned)')\n",
    "plt.xlabel('Prediksi')\n",
    "plt.ylabel('Aktual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performa model yang sudah di-pruning tetap sangat tinggi, menunjukkan bahwa aturan-aturan kompleks dari pohon yang pertama tidak diperlukan dan kemungkinan besar hanya menangkap noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 4: Analisis Pentingnya Fitur (Feature Importance)\n",
    "\n",
    "Salah satu keunggulan besar dari Decision Tree adalah kemampuannya untuk memberi tahu kita fitur mana yang paling berpengaruh dalam membuat keputusan. Ini diukur berdasarkan seberapa besar sebuah fitur mampu mengurangi *impurity* (ketidakmurnian) pada setiap pemisahan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mendapatkan skor pentingnya fitur dari model yang sudah di-pruning\n",
    "importances = tree_pruned.feature_importances_\n",
    "\n",
    "# Membuat DataFrame untuk visualisasi\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"--- Tingkat Kepentingan Fitur ---\")\n",
    "display(feature_importance_df)\n",
    "\n",
    "# Visualisasi Feature Importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\n",
    "plt.title('Pentingnya Fitur dalam Model Decision Tree', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analisis Kepentingan Fitur:**\n",
    "Hasil ini sangat konsisten dengan analisis kita di tahap *Data Understanding*. **`petal_width`** dan **`petal_length`** adalah dua fitur yang paling dominan dalam menentukan spesies bunga Iris. Fitur-fitur `sepal` memiliki kontribusi yang jauh lebih kecil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penutup dan Jembatan Konseptual\n",
    "\n",
    "Dalam notebook ini, kita telah mengupas tuntas model **Decision Tree**. Kita belajar tentang risiko overfitting dan cara mengatasinya dengan *pruning*, cara memvisualisasikan aturan model, dan cara mengekstrak informasi tentang fitur mana yang paling penting.\n",
    "\n",
    "Kita telah menyelesaikan eksplorasi tiga model **klasifikasi (supervised learning)**: Naive Bayes, KNN, dan Decision Tree. Masing-masing memiliki cara kerja dan keunggulannya sendiri.\n",
    "\n",
    "#### Jembatan ke Seri Selanjutnya\n",
    "Pada **seri selanjutnya (2. Clustering)**, kita akan beralih dari *supervised learning* ke *unsupervised learning*. Kita tidak akan lagi memprediksi label yang sudah ada ('species'), melainkan mencoba menemukan struktur atau kelompok alami dalam data tanpa menggunakan label tersebut. Kita akan memulainya dengan algoritma clustering paling populer: **K-Means**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}