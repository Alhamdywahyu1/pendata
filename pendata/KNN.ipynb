{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Pemodelan Klasifikasi â€” Deep Dive K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Jembatan Konseptual\n",
    "\n",
    "Selamat datang di bagian **1.b** dari seri pemodelan kita. Setelah pada tahap-tahap sebelumnya kita telah melakukan **Data Understanding** dan membahas berbagai teknik **Pra-Pemrosesan** (seperti penanganan outlier), di notebook ini kita akan fokus menerapkan salah satu algoritma klasifikasi paling intuitif: **K-Nearest Neighbors (KNN)**.\n",
    "\n",
    "### Filosofi KNN: \"Anda Dikenal dari Siapa Tetangga Anda\"\n",
    "\n",
    "KNN adalah algoritma yang sangat sederhana namun kuat. Prinsip kerjanya didasarkan pada asumsi bahwa data yang mirip cenderung berada berdekatan satu sama lain. Untuk mengklasifikasikan sebuah titik data baru, KNN akan:\n",
    "1.  Melihat `k` tetangga terdekatnya (berdasarkan jarak).\n",
    "2.  Mengadakan \"pemungutan suara\" (voting) di antara para tetangga tersebut.\n",
    "3.  Menetapkan kelas dari titik data baru sesuai dengan kelas mayoritas dari para tetangganya.\n",
    "\n",
    "Di notebook ini, kita akan membangun, mengevaluasi, dan yang terpenting, menemukan nilai `k` yang optimal untuk model KNN kita pada dataset Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 1: Setup Mandiri & Persiapan Data Lengkap\n",
    "\n",
    "Sebagai notebook mandiri, sel pertama ini akan menangani semua yang kita butuhkan: mengimpor pustaka, memuat data mentah, dan menerapkan seluruh alur pra-pemrosesan yang telah kita putuskan sebelumnya (standarisasi dan penghapusan outlier dengan LOF), hingga membagi data menjadi set pelatihan dan pengujian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# SETUP MANDIRI UNTUK PEMODELAN KNN\n",
    "# =======================================================\n",
    "\n",
    "# 1. Import Pustaka\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Pustaka yang dibutuhkan telah diimpor.\")\n",
    "\n",
    "# 2. Memuat dan Membuat DataFrame Awal\n",
    "iris = load_iris()\n",
    "df_full = pd.DataFrame(data=iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "df_full['species'] = [iris.target_names[i] for i in iris.target]\n",
    "print(\"Dataset Iris mentah berhasil dibuat.\")\n",
    "\n",
    "# 3. Pra-Pemrosesan: Standarisasi & Penghapusan Outlier dengan LOF\n",
    "print(\"\\nMemulai pra-pemrosesan...\")\n",
    "# Penskalaan\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(df_full[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "\n",
    "# Deteksi Outlier dengan LOF\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "outlier_labels = lof.fit_predict(features_scaled)\n",
    "df_full['outlier_lof'] = outlier_labels\n",
    "\n",
    "# Menghapus Outlier\n",
    "df_processed = df_full[df_full['outlier_lof'] == 1].copy()\n",
    "df_processed = df_processed.drop(columns=['outlier_lof'])\n",
    "print(f\"Penghapusan outlier selesai. Sisa data: {df_processed.shape[0]} baris.\")\n",
    "\n",
    "# 4. Pra-Pemrosesan: Label Encoding pada Target\n",
    "encoder = LabelEncoder()\n",
    "df_processed['species_encoded'] = encoder.fit_transform(df_processed['species'])\n",
    "print(\"Label encoding untuk variabel target selesai.\")\n",
    "\n",
    "# 5. Memisahkan Fitur (X) dan Target (y)\n",
    "X = df_processed[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "y = df_processed['species_encoded']\n",
    "\n",
    "# 6. Pembagian Data (Train-Test Split)\n",
    "# Kita perlu menskalakan ulang X setelah menghapus outlier\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"\\nPembagian data menjadi set latih dan uji selesai.\")\n",
    "print(f\"Ukuran X_train: {X_train.shape}\")\n",
    "print(f\"Ukuran X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 2: Membangun & Mengevaluasi Model KNN Dasar\n",
    "\n",
    "Untuk mendapatkan gambaran awal (baseline), kita akan membangun model KNN dengan parameter default dari Scikit-learn, yaitu `n_neighbors=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi dan latih model KNN dasar\n",
    "knn_base = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_base.fit(X_train, y_train)\n",
    "\n",
    "# Lakukan prediksi pada data uji\n",
    "y_pred_base = knn_base.predict(X_test)\n",
    "\n",
    "# Evaluasi model dasar\n",
    "print(\"--- Laporan Klasifikasi Model KNN Dasar (k=5) ---\")\n",
    "print(classification_report(y_test, y_pred_base, target_names=encoder.classes_))\n",
    "\n",
    "# Visualisasi Confusion Matrix\n",
    "cm_base = confusion_matrix(y_test, y_pred_base)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.title('Confusion Matrix - KNN Dasar (k=5)')\n",
    "plt.xlabel('Prediksi')\n",
    "plt.ylabel('Aktual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analisis Awal:** Model dasar dengan k=5 sudah menunjukkan performa yang sangat baik, dengan akurasi dan F1-score yang tinggi. Namun, apakah k=5 adalah nilai yang paling optimal? Mari kita selidiki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 3: Menemukan Nilai 'k' Optimal (Hyperparameter Tuning)\n",
    "\n",
    "Nilai `k` adalah sebuah **hyperparameter**â€”sebuah pengaturan yang kita tentukan sebelum proses pelatihan. Pemilihan `k` yang tepat sangat penting:\n",
    "* **`k` terlalu kecil:** Model menjadi sangat sensitif terhadap noise dan bisa mengalami *overfitting*.\n",
    "* **`k` terlalu besar:** Model menjadi terlalu \"umum\", kehilangan detail-detail lokal, dan bisa mengalami *underfitting*.\n",
    "\n",
    "Kita akan menggunakan **Metode Siku (Elbow Method)** untuk menemukan rentang nilai `k` yang baik secara sistematis. Idenya adalah melatih model dengan berbagai nilai `k` dan melihat pada nilai `k` berapa tingkat errornya mulai stabil (tidak turun secara signifikan lagi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List untuk menyimpan tingkat error\n",
    "error_rate = []\n",
    "\n",
    "# Mencoba nilai k dari 1 hingga 40\n",
    "k_range = range(1, 41)\n",
    "\n",
    "for k in k_range:\n",
    "    knn_loop = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_loop.fit(X_train, y_train)\n",
    "    y_pred_loop = knn_loop.predict(X_test)\n",
    "    # Menghitung error (1 - akurasi) dan menyimpannya\n",
    "    error_rate.append(np.mean(y_pred_loop != y_test))\n",
    "\n",
    "# Membuat plot Elbow Method\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(k_range, error_rate, color='blue', linestyle='--', marker='o', markersize=8, markerfacecolor='red')\n",
    "plt.title('Tingkat Error vs. Nilai K (Elbow Method)', fontsize=16)\n",
    "plt.xlabel('Nilai K')\n",
    "plt.ylabel('Tingkat Error')\n",
    "plt.xticks(np.arange(0, 41, 2))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analisis Grafik Siku:**\n",
    "Dari grafik di atas, kita dapat melihat bahwa tingkat error cukup fluktuatif pada nilai `k` yang rendah, lalu menurun dan menjadi sangat stabil setelah `k` sekitar 9 atau 11. Tingkat error terendah berada pada rentang `k` antara 9 hingga 17. Memilih nilai `k` dalam rentang ini kemungkinan akan memberikan model yang lebih stabil dan general daripada `k=5`.\n",
    "\n",
    "Untuk langkah selanjutnya, mari kita pilih **k=11** sebagai nilai optimal kita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 4: Evaluasi Akhir dengan Model KNN Optimal\n",
    "\n",
    "Sekarang kita akan melatih ulang model kita dengan `k=11` dan melihat apakah ada peningkatan performa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi dan latih model KNN optimal\n",
    "knn_optimal = KNeighborsClassifier(n_neighbors=11)\n",
    "knn_optimal.fit(X_train, y_train)\n",
    "\n",
    "# Lakukan prediksi pada data uji\n",
    "y_pred_optimal = knn_optimal.predict(X_test)\n",
    "\n",
    "# Evaluasi model optimal\n",
    "print(\"--- Laporan Klasifikasi Model KNN Optimal (k=11) ---\")\n",
    "print(classification_report(y_test, y_pred_optimal, target_names=encoder.classes_))\n",
    "\n",
    "# Visualisasi Confusion Matrix\n",
    "cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.title('Confusion Matrix - KNN Optimal (k=11)')\n",
    "plt.xlabel('Prediksi')\n",
    "plt.ylabel('Aktual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perbandingan dan Kesimpulan:**\n",
    "Dengan menggunakan `k=11`, model kita mencapai akurasi 100% pada data uji ini, sama seperti model dasar. Namun, model dengan `k` yang lebih besar (yang dipilih secara sistematis) cenderung lebih robust dan tidak terlalu sensitif terhadap data baru yang mungkin sedikit berbeda. Oleh karena itu, model dengan `k=11` adalah pilihan yang lebih baik secara teoritis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penutup dan Jembatan Konseptual\n",
    "\n",
    "Dalam notebook ini, kita telah melakukan analisis mendalam terhadap model **K-Nearest Neighbors**. Kita tidak hanya membangun model, tetapi juga memahami pentingnya hyperparameter `k` dan menggunakan *Elbow Method* untuk menemukannya secara sistematis.\n",
    "\n",
    "Kita telah berhasil membangun sebuah model klasifikasi KNN yang optimal untuk dataset Iris yang telah kita proses.\n",
    "\n",
    "**Pada bagian selanjutnya, [1.c Klasifikasi Decision Tree](./1.c_Klasifikasi_Tree.ipynb)**, kita akan beralih ke algoritma yang bekerja dengan logika yang sama sekali berbeda. Jika KNN berbasis kedekatan (jarak), Decision Tree bekerja dengan membuat serangkaian aturan 'jika-maka' untuk mencapai sebuah keputusan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}