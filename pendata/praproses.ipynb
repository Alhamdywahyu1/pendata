{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚙️ Tahap 2: Pra-Pemrosesan Data\n",
    "\n",
    "### Jembatan Konseptual dari Tahap Sebelumnya\n",
    "\n",
    "Pada **Tahap 1 (Data Understanding)**, kita telah membedah dataset Iris untuk memahami karakteristik fundamentalnya. Analisis tersebut memberikan kita fondasi yang kuat, namun juga menyoroti adanya potensi **pencilan (outlier)**—data yang perilakunya menyimpang dari norma.\n",
    "\n",
    "### Filosofi Pra-Pemrosesan: Prinsip \"Garbage In, Garbage Out\"\n",
    "\n",
    "Sebelum melangkah ke pemodelan, kita harus memegang teguh prinsip ini. Kualitas sebuah model machine learning tidak akan pernah bisa melebihi kualitas data yang digunakan untuk melatihnya. Pencilan dapat secara drastis mengganggu proses pelatihan:\n",
    "* **Menarik Garis Regresi:** Dalam model linear, satu pencilan saja dapat mengubah kemiringan garis secara signifikan.\n",
    "* **Meningkatkan Varians:** Pencilan meningkatkan varians data, membuat model lebih sulit menemukan pola yang sebenarnya.\n",
    "* **Melanggar Asumsi:** Banyak algoritma (terutama yang berbasis statistik) memiliki asumsi tentang distribusi data yang dapat dilanggar oleh pencilan.\n",
    "\n",
    "Di notebook ini, kita akan melakukan penyelidikan mendalam untuk mendeteksi dan menganalisis pencilan menggunakan dua metode canggih, **Local Outlier Factor (LOF)** dan **K-Nearest Neighbors (KNN)**, untuk memastikan data yang kita siapkan untuk pemodelan memiliki kualitas tertinggi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 2.1: Setup Mandiri dan Persiapan Data\n",
    "\n",
    "Sebagai notebook yang mandiri, kita akan memulai dengan mengimpor semua pustaka yang diperlukan dan membuat ulang DataFrame `df_iris` dari awal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# SETUP MANDIRI UNTUK TAHAP 2\n",
    "# =======================================================\n",
    "\n",
    "# Import pustaka yang dibutuhkan untuk keseluruhan notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
    "\n",
    "# Mengatur gaya visualisasi global\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "# --- Membuat ulang DataFrame Iris dari awal ---\n",
    "iris = load_iris()\n",
    "df_full = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df_full['species'] = [iris.target_names[i] for i in iris.target]\n",
    "df_full['id'] = range(1, len(df_full) + 1)\n",
    "\n",
    "# Ganti nama kolom agar lebih sederhana\n",
    "df_full.rename(columns={\n",
    "    'sepal length (cm)': 'sepal_length', 'sepal width (cm)': 'sepal_width',\n",
    "    'petal length (cm)': 'petal_length', 'petal width (cm)': 'petal_width'\n",
    "}, inplace=True)\n",
    "\n",
    "df_iris = df_full[['id', 'species', 'sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "\n",
    "print(\"Setup mandiri selesai. DataFrame 'df_iris' siap untuk pra-pemrosesan.\")\n",
    "display(df_iris.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 2.2: Penskalaan Fitur — Perbandingan Mendalam\n",
    "\n",
    "Penskalaan fitur adalah fondasi dari deteksi outlier berbasis jarak. Mari kita bandingkan dua metode populer secara visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Standarisasi (Standard Scaler)\n",
    "Mengubah data sehingga berpusat di rata-rata 0 dengan standar deviasi 1. Formula: $z = (x - \\mu) / \\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_standard = StandardScaler()\n",
    "df_standardized = pd.DataFrame(scaler_standard.fit_transform(df_iris.select_dtypes(include=np.number)), columns=df_iris.select_dtypes(include=np.number).columns)\n",
    "\n",
    "# Visualisasi Sebelum vs Sesudah Standarisasi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.kdeplot(data=df_iris, ax=axes[0]).set_title('Sebelum Standarisasi')\n",
    "sns.kdeplot(data=df_standardized, ax=axes[1]).set_title('Sesudah Standarisasi')\n",
    "plt.suptitle('Efek Standarisasi pada Distribusi Fitur', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "display(df_standardized.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terlihat bahwa setelah standarisasi, rata-rata (`mean`) setiap fitur menjadi mendekati 0 dan standar deviasinya (`std`) menjadi 1, namun bentuk distribusinya tetap dipertahankan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Normalisasi (Min-Max Scaler)\n",
    "Mengubah data sehingga nilainya berada dalam rentang [0, 1]. Formula: $X_{norm} = (X - X_{min}) / (X_{max} - X_{min})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_minmax = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler_minmax.fit_transform(df_iris.select_dtypes(include=np.number)), columns=df_iris.select_dtypes(include=np.number).columns)\n",
    "\n",
    "# Visualisasi Sebelum vs Sesudah Normalisasi\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.kdeplot(data=df_iris, ax=axes[0]).set_title('Sebelum Normalisasi')\n",
    "sns.kdeplot(data=df_normalized, ax=axes[1]).set_title('Sesudah Normalisasi')\n",
    "plt.suptitle('Efek Normalisasi pada Distribusi Fitur', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "display(df_normalized.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah normalisasi, nilai minimum menjadi 0 dan nilai maksimum menjadi 1 untuk setiap fitur.\n",
    "\n",
    "**Keputusan:** Untuk deteksi outlier dengan LOF dan KNN, **Standarisasi** seringkali lebih disukai karena mempertahankan informasi tentang varians dan tidak \"memaksa\" data ke dalam rentang yang sempit. Kita akan menggunakan `df_standardized` untuk analisis selanjutnya (namun tanpa kolom `id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita akan menggunakan data hasil standarisasi untuk deteksi outlier\n",
    "df_scaled = df_standardized.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 2.3: Analisis Mendalam — Local Outlier Factor (LOF)\n",
    "\n",
    "LOF mengukur tingkat anomali suatu titik data dengan membandingkan kepadatan lokalnya terhadap kepadatan lokal para tetangganya. Skor LOF sekitar 1 berarti titik tersebut memiliki kepadatan serupa dengan tetangganya (inlier). Skor yang jauh lebih besar dari 1 menunjukkan titik tersebut berada di area yang jauh lebih jarang penduduknya daripada tetangganya (outlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eksperimen dengan Parameter `n_neighbors` (k)\n",
    "Parameter `n_neighbors` atau `k` sangat memengaruhi hasil LOF. Mari kita lihat bagaimana deteksi outlier berubah dengan nilai `k` yang berbeda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [5, 10, 20, 30]\n",
    "fig, axes = plt.subplots(1, len(k_values), figsize=(20, 5), sharey=True)\n",
    "\n",
    "df_iris_lof = df_iris.copy()\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "    lof = LocalOutlierFactor(n_neighbors=k)\n",
    "    outlier_labels = lof.fit_predict(df_scaled)\n",
    "    \n",
    "    # Simpan label untuk nilai k=20 (standar) untuk analisis nanti\n",
    "    if k == 20:\n",
    "        df_iris_lof['outlier_lof'] = outlier_labels\n",
    "\n",
    "    # Visualisasi\n",
    "    scatter = sns.scatterplot(\n",
    "        x=df_scaled['petal_length'], \n",
    "        y=df_scaled['petal_width'], \n",
    "        hue=outlier_labels, \n",
    "        palette={1: 'blue', -1: 'red'}, \n",
    "        style=outlier_labels, \n",
    "        markers={1: 'o', -1: 'X'},\n",
    "        s=100,\n",
    "        ax=axes[i]\n",
    "    )\n",
    "    axes[i].set_title(f'LOF dengan k={k}')\n",
    "    axes[i].legend_.remove()\n",
    "\n",
    "plt.suptitle('Perbandingan Deteksi Outlier LOF dengan Nilai k Berbeda', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analisis Eksperimen:**\n",
    "* **k kecil (5, 10):** Cenderung lebih sensitif dan mendeteksi lebih banyak outlier 'lokal' yang mungkin tidak terlalu signifikan.\n",
    "* **k besar (20, 30):** Memberikan pandangan yang lebih stabil dan global. Nilai `k=20` (default) seringkali merupakan titik awal yang baik.\n",
    "\n",
    "Kita akan melanjutkan analisis dengan hasil dari `k=20`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis Skor LOF\n",
    "Atribut `negative_outlier_factor_` memberikan skor mentah LOF (semakin kecil/negatif, semakin besar kemungkinan outlier). Mari kita visualisasikan distribusinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan hasil dari k=20\n",
    "lof_final = LocalOutlierFactor(n_neighbors=20)\n",
    "lof_final.fit(df_scaled)\n",
    "scores = lof_final.negative_outlier_factor_\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(scores, bins=30, kde=True)\n",
    "plt.title('Distribusi Skor LOF (Negative Outlier Factor)')\n",
    "plt.xlabel('Skor LOF')\n",
    "plt.show()\n",
    "\n",
    "print(\"Skor LOF terendah (paling anomali):\")\n",
    "print(np.sort(scores)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafik ini menunjukkan bahwa sebagian besar titik data memiliki skor di sekitar -1.0, sementara ada beberapa titik dengan skor yang jauh lebih rendah, yang memperkuat identifikasi mereka sebagai pencilan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 2.4: Analisis Mendalam — KNN untuk Deteksi Pencilan\n",
    "\n",
    "Metode ini menggunakan jarak ke tetangga terdekat sebagai proksi untuk anomali. Ide dasarnya: titik normal akan dekat dengan tetangganya, sementara pencilan akan jauh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisasi Distribusi Jarak ke Tetangga ke-k\n",
    "Langkah pertama adalah menghitung jarak setiap titik ke tetangga ke-`k` nya. Distribusi jarak ini akan memberi kita gambaran tentang ambang batas yang wajar untuk sebuah pencilan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "neighbors_knn = NearestNeighbors(n_neighbors=k).fit(df_scaled)\n",
    "distances, indices = neighbors_knn.kneighbors(df_scaled)\n",
    "\n",
    "# Jarak ke tetangga ke-k (paling jauh dalam kelompok k)\n",
    "k_th_distances = distances[:, k-1]\n",
    "\n",
    "# Tentukan ambang batas berdasarkan persentil ke-95\n",
    "threshold = np.percentile(k_th_distances, 95)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(k_th_distances, bins=30, kde=True)\n",
    "plt.axvline(x=threshold, color='red', linestyle='--', label=f'Ambang Batas (95th Percentile = {threshold:.2f})')\n",
    "plt.title(f'Distribusi Jarak ke Tetangga ke-{k}')\n",
    "plt.xlabel('Jarak')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafik di atas menunjukkan distribusi jarak. Titik-titik di sebelah kanan garis merah (ambang batas) adalah yang akan kita klasifikasikan sebagai pencilan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifikasi outlier berdasarkan ambang batas\n",
    "df_iris_knn = df_iris.copy()\n",
    "df_iris_knn['outlier_knn'] = k_th_distances >= threshold\n",
    "\n",
    "print(f\"Jumlah outlier yang terdeteksi oleh KNN: {df_iris_knn['outlier_knn'].sum()}\")\n",
    "print(\"\\nData yang teridentifikasi sebagai outlier oleh KNN:\")\n",
    "display(df_iris_knn[df_iris_knn['outlier_knn'] == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 2.5: Perbandingan, Keputusan, dan Penutup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perbandingan Langsung LOF vs. KNN\n",
    "Mari kita lihat titik data mana saja yang ditandai oleh kedua metode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggabungkan hasil deteksi dari kedua metode\n",
    "df_comparison = pd.DataFrame({\n",
    "    'LOF_Outlier': df_iris_lof['outlier_lof'] == -1,\n",
    "    'KNN_Outlier': df_iris_knn['outlier_knn']\n",
    "})\n",
    "\n",
    "# Membuat tabel kontingensi\n",
    "comparison_table = pd.crosstab(df_comparison['LOF_Outlier'], df_comparison['KNN_Outlier'])\n",
    "print(\"--- Tabel Perbandingan Deteksi Outlier ---\")\n",
    "display(comparison_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretasi Tabel:**\n",
    "* **135 titik** dianggap normal oleh kedua metode (False, False).\n",
    "* **3 titik** dianggap outlier oleh keduanya (True, True).\n",
    "* **5 titik** dianggap outlier oleh KNN saja.\n",
    "* **7 titik** dianggap outlier oleh LOF saja.\n",
    "\n",
    "Ini menunjukkan adanya kesamaan tetapi juga perbedaan, yang wajar karena kedua algoritma mengukur 'keanehan' dengan cara yang berbeda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keputusan Akhir dan Tindak Lanjut\n",
    "\n",
    "Untuk tujuan proyek ini, kita perlu memilih satu set data yang 'bersih' untuk dilanjutkan ke tahap pemodelan. LOF sering dianggap lebih unggul karena kemampuannya menangani cluster dengan kepadatan yang bervariasi. Oleh karena itu, kita akan membuat keputusan untuk melanjutkan dengan data yang telah dibersihkan dari outlier yang terdeteksi oleh **LOF (dengan k=20)**.\n",
    "\n",
    "Langkah terakhir adalah memfilter dataset berdasarkan keputusan ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat DataFrame akhir yang bersih berdasarkan hasil LOF\n",
    "df_final_processed = df_iris_lof[df_iris_lof['outlier_lof'] == 1].copy()\n",
    "\n",
    "# Menghapus kolom helper 'outlier_lof'\n",
    "df_final_processed = df_final_processed.drop(columns=['outlier_lof'])\n",
    "\n",
    "print(\"--- Proses Pembersihan Akhir Selesai ---\")\n",
    "print(f\"Ukuran dataset asli: {df_iris.shape}\")\n",
    "print(f\"Ukuran dataset setelah menghapus outlier (LOF): {df_final_processed.shape}\")\n",
    "display(df_final_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jembatan ke Tahap Berikutnya\n",
    "\n",
    "Dengan selesainya notebook ini, kita telah melakukan analisis pra-pemrosesan yang sangat mendalam. Kita tidak hanya menerapkan teknik, tetapi juga bereksperimen dengan parameter dan membandingkan hasilnya.\n",
    "\n",
    "Hasil akhir kita adalah `df_final_processed`, sebuah dataset yang telah diskalakan dan dibersihkan dari pencilan yang paling signifikan. Data ini sekarang berada dalam kondisi optimal.\n",
    "\n",
    "Pada **Tahap 3: Pemodelan**, kita akan menggunakan konsep pembersihan ini (memuat data mentah, menskalakan, dan menghapus outlier) sebagai langkah pertama sebelum melatih, menguji, dan membandingkan berbagai model klasifikasi untuk memprediksi spesies Iris."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}