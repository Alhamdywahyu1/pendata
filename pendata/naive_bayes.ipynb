{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Pemodelan Klasifikasi â€” Deep Dive Naive Bayes\n",
    "\n",
    "### Jembatan Konseptual\n",
    "\n",
    "Selamat datang di bagian **1.a**, bagian pertama dari seri pemodelan klasifikasi kita. Setelah pada tahap-tahap sebelumnya kita telah melakukan **Data Understanding** dan **Pra-Pemrosesan**, kita kini siap untuk memulai pemodelan.\n",
    "\n",
    "Kita akan memulai dengan model yang elegan dan berbasis probabilitas: **Naive Bayes**.\n",
    "\n",
    "### Filosofi Naive Bayes: \"Probabilitas Bersyarat\"\n",
    "\n",
    "Naive Bayes bekerja berdasarkan **Teorema Bayes**, sebuah prinsip fundamental dalam teori probabilitas. \n",
    "\n",
    "> **Analogi Sederhana:** Bayangkan seorang dokter yang mendiagnosis penyakit. Dokter tersebut tidak hanya melihat gejala yang ada pada pasien (`P(Gejala|Penyakit)`), tetapi juga mempertimbangkan seberapa umum penyakit itu di populasi secara umum (`P(Penyakit)`). Dengan menggabungkan dua informasi ini, dokter dapat membuat diagnosis yang lebih akurat (`P(Penyakit|Gejala)`).\n",
    "\n",
    "Model ini disebut **'naive' (naif)** karena ia membuat asumsi yang menyederhanakan masalah: yaitu bahwa semua fitur (misalnya, `sepal_length` dan `petal_width`) bersifat **independen** satu sama lain. Meskipun asumsi ini jarang sekali benar di dunia nyata, Naive Bayes seringkali bekerja dengan sangat baik dan cepat.\n",
    "\n",
    "Di notebook ini, kita akan membangun, mengevaluasi, dan \"membedah\" isi dari model Gaussian Naive Bayes untuk benar-benar memahami cara kerjanya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 1: Setup Mandiri & Persiapan Data Lengkap\n",
    "\n",
    "Sebagai notebook mandiri, sel pertama ini akan menangani semua yang kita butuhkan: mengimpor pustaka, memuat data mentah, dan menerapkan seluruh alur pra-pemrosesan yang telah kita putuskan sebelumnya (standarisasi dan penghapusan outlier dengan LOF), hingga membagi data menjadi set pelatihan dan pengujian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# SETUP MANDIRI UNTUK PEMODELAN NAIVE BAYES\n",
    "# =======================================================\n",
    "\n",
    "# 1. Import Pustaka\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"Pustaka yang dibutuhkan telah diimpor.\")\n",
    "\n",
    "# 2. Memuat dan Membuat DataFrame Awal\n",
    "iris = load_iris()\n",
    "df_full = pd.DataFrame(data=iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "df_full['species'] = [iris.target_names[i] for i in iris.target]\n",
    "print(\"Dataset Iris mentah berhasil dibuat.\")\n",
    "\n",
    "# 3. Pra-Pemrosesan: Standarisasi & Penghapusan Outlier dengan LOF\n",
    "print(\"\\nMemulai pra-pemrosesan...\")\n",
    "# Penskalaan (diperlukan untuk LOF dan bisa membantu asumsi Gaussian pada Naive Bayes)\n",
    "features_to_scale = df_full[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_to_scale)\n",
    "\n",
    "# Deteksi Outlier dengan LOF\n",
    "lof = LocalOutlierFactor(n_neighbors=20)\n",
    "outlier_labels = lof.fit_predict(features_scaled)\n",
    "df_full['outlier_lof'] = outlier_labels\n",
    "\n",
    "# Menghapus Outlier\n",
    "df_processed = df_full[df_full['outlier_lof'] == 1].copy()\n",
    "df_processed = df_processed.drop(columns=['outlier_lof'])\n",
    "print(f\"Penghapusan outlier selesai. Sisa data: {df_processed.shape[0]} baris.\")\n",
    "\n",
    "# 4. Pra-Pemrosesan: Label Encoding pada Target\n",
    "encoder = LabelEncoder()\n",
    "df_processed['species_encoded'] = encoder.fit_transform(df_processed['species'])\n",
    "print(\"Label encoding untuk variabel target selesai.\")\n",
    "\n",
    "# 5. Memisahkan Fitur (X) dan Target (y)\n",
    "# Kita akan menggunakan data yang sudah diskalakan\n",
    "X_unscaled = df_processed[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "X = scaler.fit_transform(X_unscaled) # Skalakan ulang pada data yang sudah bersih\n",
    "y = df_processed['species_encoded']\n",
    "\n",
    "# 6. Pembagian Data (Train-Test Split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print(\"\\nPembagian data menjadi set latih dan uji selesai.\")\n",
    "print(f\"Ukuran X_train: {X_train.shape}\")\n",
    "print(f\"Ukuran X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 2: Membangun & Mengevaluasi Model Naive Bayes\n",
    "\n",
    "Kita akan menggunakan varian `GaussianNB` dari Scikit-learn, yang cocok untuk fitur-fitur kontinu (seperti ukuran sepal dan petal) karena ia mengasumsikan bahwa fitur untuk setiap kelas mengikuti distribusi normal (Gaussian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi dan latih model Gaussian Naive Bayes\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Lakukan prediksi pada data uji\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluasi model\n",
    "print(\"--- Laporan Klasifikasi Model Gaussian Naive Bayes ---\")\n",
    "print(classification_report(y_test, y_pred, target_names=encoder.classes_))\n",
    "\n",
    "# Visualisasi Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\n",
    "plt.title('Confusion Matrix - Gaussian Naive Bayes')\n",
    "plt.xlabel('Prediksi')\n",
    "plt.ylabel('Aktual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analisis Awal:** Model Naive Bayes menunjukkan performa yang sangat baik, dengan akurasi tinggi dan hanya sedikit kesalahan klasifikasi antara `versicolor` dan `virginica`, sesuai dengan yang kita harapkan dari analisis data awal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langkah 3: \"Deep Dive\" â€” Membedah Isi Model Naive Bayes\n",
    "\n",
    "Keindahan Naive Bayes adalah kita bisa dengan mudah melihat apa yang telah ia \"pelajari\" dari data. Mari kita lihat ke dalam model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Menganalisis Probabilitas Prior (`class_prior_`)\n",
    "Ini adalah probabilitas awal dari setiap kelas, yang dihitung dari proporsi masing-masing kelas di data latihan. Ini adalah `P(Penyakit)` dalam analogi dokter kita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = nb_model.class_prior_\n",
    "prior_df = pd.DataFrame(priors, index=encoder.classes_, columns=['Probabilitas Prior'])\n",
    "\n",
    "print(\"--- Probabilitas Prior yang Dipelajari Model ---\")\n",
    "display(prior_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena data kita seimbang, probabilitas prior untuk setiap kelas hampir sama (sekitar 33%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Menganalisis Parameter Distribusi (`theta_` dan `var_`)\n",
    "`GaussianNB` mempelajari rata-rata (`theta_`) dan varians (`var_`) untuk setiap fitur di setiap kelas. Parameter-parameter inilah yang mendefinisikan kurva lonceng (distribusi normal) untuk setiap kombinasi fitur-kelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = nb_model.theta_\n",
    "variances = nb_model.var_\n",
    "feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "params_df = pd.DataFrame(means, columns=[f'{name}_mean' for name in feature_names])\n",
    "params_df = pd.concat([\n",
    "    params_df, \n",
    "    pd.DataFrame(variances, columns=[f'{name}_var' for name in feature_names])\n",
    "], axis=1)\n",
    "params_df.index = encoder.classes_\n",
    "\n",
    "print(\"--- Rata-rata (theta) dan Varians (var) yang Dipelajari Model ---\")\n",
    "display(params_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Visualisasi Distribusi Gaussian yang Dipelajari Model\n",
    "Ini adalah bagian yang paling menarik. Mari kita visualisasikan kurva lonceng yang telah dipelajari model menggunakan parameter rata-rata dan varians di atas, dan kita bandingkan dengan distribusi data yang sebenarnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pilih fitur untuk divisualisasikan, misal 'petal_length'\n",
    "feature_index = 2 # Indeks untuk petal_length\n",
    "feature_name = feature_names[feature_index]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "for i, class_name in enumerate(encoder.classes_):\n",
    "    # Dapatkan parameter yang dipelajari model\n",
    "    mean = nb_model.theta_[i, feature_index]\n",
    "    std_dev = np.sqrt(nb_model.var_[i, feature_index])\n",
    "    \n",
    "    # Buat rentang nilai untuk plot\n",
    "    x_axis = np.linspace(mean - 4*std_dev, mean + 4*std_dev, 1000)\n",
    "    # Hitung PDF (Probability Density Function)\n",
    "    pdf = stats.norm.pdf(x_axis, loc=mean, scale=std_dev)\n",
    "    \n",
    "    # Plot kurva Gaussian yang dipelajari model\n",
    "    plt.plot(x_axis, pdf, label=f'Model Gaussian untuk {class_name}')\n",
    "    \n",
    "    # Plot distribusi data asli untuk perbandingan\n",
    "    sns.kdeplot(X_train[y_train == i, feature_index], label=f'Data Asli {class_name}', fill=True, alpha=0.2)\n",
    "\n",
    "plt.title(f'Distribusi Gaussian yang Dipelajari Model vs. Data Asli untuk Fitur: {feature_name.replace(\"_\", \" \").title()}', fontsize=16)\n",
    "plt.xlabel(f'{feature_name.replace(\"_\", \" \").title()} (Scaled)')\n",
    "plt.ylabel('Kepadatan Probabilitas (Density)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analisis Visualisasi:**\n",
    "Grafik ini secara visual menunjukkan bagaimana Naive Bayes bekerja. Garis tebal adalah kurva probabilitas yang \"diyakini\" oleh model, sedangkan area yang diarsir adalah distribusi data yang sebenarnya.\n",
    "* Kurva untuk **setosa** sangat terpisah dari yang lain, menjelaskan mengapa model tidak pernah salah mengklasifikasikannya.\n",
    "* Kurva untuk **versicolor** dan **virginica** memiliki tumpang tindih yang signifikan. Di area tumpang tindih inilah model membuat keputusan berdasarkan probabilitas prior dan bukti dari fitur-fitur lain. Ini juga menjelaskan mengapa kesalahan klasifikasi terjadi di antara kedua kelas ini.\n",
    "Secara umum, asumsi distribusi normal dari model (garis tebal) cukup cocok dengan distribusi data asli (area arsir)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penutup dan Jembatan Konseptual\n",
    "\n",
    "Dalam notebook ini, kita tidak hanya membangun model **Naive Bayes** tetapi juga melakukan penyelidikan mendalam ke dalam cara kerjanya. Kita melihat bagaimana probabilitas prior dan parameter distribusi Gaussian digunakan untuk membuat keputusan klasifikasi. Naive Bayes terbukti menjadi model yang sangat cepat dan efektif untuk dataset ini.\n",
    "\n",
    "#### Jembatan ke Seri Selanjutnya\n",
    "\n",
    "Setelah memahami model probabilistik ini, pada **[bagian selanjutnya (1.b)](./1.b_Klasifikasi_KNN.ipynb)**, kita akan melihat pendekatan yang sama sekali berbeda dengan **K-Nearest Neighbors**, yang tidak membuat asumsi distribusi apa pun tentang data dan bekerja murni berdasarkan kedekatan jarak antar sampel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}